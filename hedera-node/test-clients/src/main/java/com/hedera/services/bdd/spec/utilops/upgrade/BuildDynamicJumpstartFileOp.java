// SPDX-License-Identifier: Apache-2.0
package com.hedera.services.bdd.spec.utilops.upgrade;

import static com.hedera.node.app.hapi.utils.CommonUtils.sha384DigestOrThrow;
import static com.hedera.services.bdd.junit.hedera.ExternalPath.WRAPPED_RECORD_HASHES_FILE;
import static com.hedera.services.bdd.junit.hedera.utils.WorkingDirUtils.DATA_DIR;
import static com.hedera.services.bdd.junit.hedera.utils.WorkingDirUtils.guaranteedExtantDir;
import static java.util.Objects.requireNonNull;

import com.hedera.hapi.block.internal.WrappedRecordFileBlockHashesLog;
import com.hedera.node.app.blocks.impl.IncrementalStreamingHasher;
import com.hedera.pbj.runtime.io.buffer.Bytes;
import com.hedera.services.bdd.spec.HapiSpec;
import com.hedera.services.bdd.spec.utilops.UtilOp;
import edu.umd.cs.findbugs.annotations.NonNull;
import java.io.ByteArrayOutputStream;
import java.io.DataOutputStream;
import java.nio.file.Files;
import java.util.List;
import java.util.Random;
import java.util.concurrent.atomic.AtomicReference;

/**
 * A utility operation that dynamically generates a valid jumpstart binary file and writes it to the
 * {@code cutover} directory of every node in the target network.
 *
 * This operation reads wrapped record hashes already on disk to determine the correct jumpstart block
 * number, then builds a hasher state by seeding it with {@value #SIMULATED_HISTORICAL_HASH_COUNT}
 * random 48-byte hashes (simulating blocks that pre-date the jumpstart window).
 *
 * <p>The generated bytes are also exposed via an {@link AtomicReference} so that downstream
 * operations (e.g. {@link VerifyJumpstartHashOp}) can independently replay and verify the
 * computation.
 */
public class BuildDynamicJumpstartFileOp extends UtilOp {
    private static final String CUTOVER_DIR = "cutover";
    private static final String JUMPSTART_FILENAME = "jumpstart.bin";
    private static final int SIMULATED_HISTORICAL_HASH_COUNT = 100;

    private final AtomicReference<byte[]> contentsRef;

    public BuildDynamicJumpstartFileOp(@NonNull final AtomicReference<byte[]> contentsRef) {
        this.contentsRef = requireNonNull(contentsRef);
    }

    @Override
    protected boolean submitOp(@NonNull final HapiSpec spec) throws Throwable {
        final var nodes = spec.targetNetworkOrThrow().nodes();

        // Read the first node's wrapped record hashes file to determine the jumpstart block number
        final var firstNode = nodes.getFirst();
        final var hashesFile = firstNode.getExternalPath(WRAPPED_RECORD_HASHES_FILE);
        final var allBytes = Files.readAllBytes(hashesFile);
        final var log = WrappedRecordFileBlockHashesLog.PROTOBUF.parse(Bytes.wrap(allBytes));
        final var entries = log.entries();
        if (entries.isEmpty()) {
            throw new IllegalStateException("Wrapped record hashes file is empty on node " + firstNode.getNodeId());
        }
        final long jumpstartBlockNum = entries.getFirst().blockNumber();

        // Build a pseudo-realistic hasher state with SIMULATED_HISTORICAL_HASH_COUNT random hashes, simulating a
        // network that has been running for some time before the jumpstart window.
        final var hasher = new IncrementalStreamingHasher(sha384DigestOrThrow(), List.of(), 0L);
        final var rng = new Random();
        byte[] prevHash = null;
        for (int i = 0; i < SIMULATED_HISTORICAL_HASH_COUNT; i++) {
            final var randomHash = new byte[48];
            rng.nextBytes(randomHash);
            hasher.addNodeByHash(randomHash);

            if (i == SIMULATED_HISTORICAL_HASH_COUNT - 1) {
                prevHash = randomHash;
            }
        }
        requireNonNull(prevHash, "Previous hash must be assigned");

        // Serialize to the jumpstart binary format expected by WrappedRecordBlockHashMigration:
        //   8 bytes  : block number (long)
        //   48 bytes : previous block root hash (SHA-384)
        //   8 bytes  : streaming hasher leaf count (long)
        //   4 bytes  : streaming hasher hash count (int)
        //   48 bytes * hash count : streaming hasher pending subtree hashes
        final var baos = new ByteArrayOutputStream();
        try (final var dout = new DataOutputStream(baos)) {
            dout.writeLong(jumpstartBlockNum);
            dout.write(prevHash);
            dout.writeLong(hasher.leafCount());
            final var intermediateHashes = hasher.intermediateHashingState();
            dout.writeInt(intermediateHashes.size());
            for (final var hash : intermediateHashes) {
                dout.write(hash.toByteArray());
            }
        }
        final var jumpstartBytes = baos.toByteArray();

        // Write jumpstart file to every node's cutover directory
        for (final var node : nodes) {
            final var workingDir = requireNonNull(node.metadata().workingDir());
            final var cutoverDir =
                    guaranteedExtantDir(workingDir.resolve(DATA_DIR).resolve(CUTOVER_DIR));
            Files.write(cutoverDir.resolve(JUMPSTART_FILENAME), jumpstartBytes);
        }

        // Expose the generated bytes for downstream verification
        contentsRef.set(jumpstartBytes);
        return false;
    }
}
