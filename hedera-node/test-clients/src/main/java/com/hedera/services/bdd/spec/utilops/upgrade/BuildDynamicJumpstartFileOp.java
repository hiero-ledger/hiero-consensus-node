// SPDX-License-Identifier: Apache-2.0
package com.hedera.services.bdd.spec.utilops.upgrade;

import static com.hedera.node.app.blocks.BlockStreamManager.HASH_OF_ZERO;
import static com.hedera.node.app.blocks.impl.BlockImplUtils.hashInternalNode;
import static com.hedera.node.app.blocks.impl.BlockImplUtils.hashInternalNodeSingleChild;
import static com.hedera.node.app.hapi.utils.CommonUtils.sha384DigestOrThrow;
import static com.hedera.services.bdd.junit.hedera.ExternalPath.WRAPPED_RECORD_HASHES_FILE;
import static com.hedera.services.bdd.junit.hedera.utils.WorkingDirUtils.DATA_DIR;
import static com.hedera.services.bdd.junit.hedera.utils.WorkingDirUtils.guaranteedExtantDir;
import static java.util.Objects.requireNonNull;

import com.hedera.hapi.block.internal.WrappedRecordFileBlockHashes;
import com.hedera.hapi.block.internal.WrappedRecordFileBlockHashesLog;
import com.hedera.node.app.blocks.impl.IncrementalStreamingHasher;
import com.hedera.node.app.records.impl.WrappedRecordBlockHashMigration;
import com.hedera.pbj.runtime.io.buffer.Bytes;
import com.hedera.services.bdd.spec.HapiSpec;
import com.hedera.services.bdd.spec.utilops.UtilOp;
import edu.umd.cs.findbugs.annotations.NonNull;
import java.io.ByteArrayOutputStream;
import java.io.DataOutputStream;
import java.nio.file.Files;
import java.util.List;
import java.util.concurrent.atomic.AtomicReference;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * A utility operation that dynamically generates a valid jumpstart binary file and writes it to the
 * {@code cutover} directory of every node in the target network.
 *
 * <p>This operation reads the wrapped record hashes already on disk, partitions them into two
 * roughly equal halves, and uses the <em>first half</em> to compute real block hashes (using the
 * same algorithm as {@link WrappedRecordBlockHashMigration}) to build a genuine jumpstart state.
 * The jumpstart file is written at the midpoint (the last block of the first half), so the
 * migration on restart will process only the <em>second half</em> of the wrapped record entries.
 *
 * <p>The generated bytes are also exposed via an {@link AtomicReference} so that downstream
 * operations (e.g. {@link VerifyJumpstartHashOp}) can independently replay and verify the result.
 */
public class BuildDynamicJumpstartFileOp extends UtilOp {
    private static final Logger log = LogManager.getLogger(BuildDynamicJumpstartFileOp.class);

    private static final String CUTOVER_DIR = "cutover";
    private static final String JUMPSTART_FILENAME = "jumpstart.bin";
    private static final Bytes EMPTY_INT_NODE = hashInternalNode(HASH_OF_ZERO, HASH_OF_ZERO);

    private final AtomicReference<byte[]> contentsRef;

    public BuildDynamicJumpstartFileOp(@NonNull final AtomicReference<byte[]> contentsRef) {
        this.contentsRef = requireNonNull(contentsRef);
    }

    @Override
    protected boolean submitOp(@NonNull final HapiSpec spec) throws Throwable {
        final var nodes = spec.targetNetworkOrThrow().nodes();

        // Read the wrapped record hashes from node 0
        final var node0 = nodes.getFirst();
        final var hashesFile = node0.getExternalPath(WRAPPED_RECORD_HASHES_FILE);
        final var allBytes = Files.readAllBytes(hashesFile);
        final var hashesLog = WrappedRecordFileBlockHashesLog.PROTOBUF.parse(Bytes.wrap(allBytes));
        final var entries = hashesLog.entries();
        if (entries.size() < 2) {
            throw new IllegalStateException(
                    "Need at least 2 wrapped record entries to build a jumpstart file, but found " + entries.size()
                            + " on node " + node0.getNodeId());
        }
        log.info(
                "Loaded {} wrapped record entries from {}. Range in file: {} to {}",
                entries.size(),
                hashesFile,
                entries.getFirst().blockNumber(),
                entries.getLast().blockNumber());

        // Partition into two roughly equal halves. The jumpstart file will represent the
        // completed state at the last block of the first half; the migration on restart
        // will process only the second half (entries with blockNumber > jumpstartBlockNum).
        final int mid = entries.size() / 2;
        final List<WrappedRecordFileBlockHashes> firstHalf = entries.subList(0, mid);
        log.info(
                "Partitioned wrapped block records into firstHalf=[{}, {}], secondHalf=[{}, {}]",
                firstHalf.getFirst().blockNumber(),
                firstHalf.getLast().blockNumber(),
                entries.get(mid).blockNumber(),
                entries.getLast().blockNumber());

        // Process the first half using the same hash computation as
        // WrappedRecordBlockHashMigration.computeAndWriteHashes(), starting from the
        // SHA-384 hash of a 48-byte zero array as the initial "previous block hash".
        final var allPrevBlocksHasher = new IncrementalStreamingHasher(sha384DigestOrThrow(), List.of(), 0L);
        Bytes prevWrappedBlockHash = HASH_OF_ZERO;
        for (final var entry : firstHalf) {
            final Bytes allPrevBlocksHash = Bytes.wrap(allPrevBlocksHasher.computeRootHash());
            final Bytes depth5Node1 = hashInternalNode(prevWrappedBlockHash, allPrevBlocksHash);
            final Bytes depth5Node2 = EMPTY_INT_NODE;
            final Bytes depth5Node3 = hashInternalNode(HASH_OF_ZERO, entry.outputItemsTreeRootHash());
            final Bytes depth5Node4 = EMPTY_INT_NODE;

            final Bytes depth4Node1 = hashInternalNode(depth5Node1, depth5Node2);
            final Bytes depth4Node2 = hashInternalNode(depth5Node3, depth5Node4);
            final Bytes depth3Node1 = hashInternalNode(depth4Node1, depth4Node2);

            final Bytes depth2Node1 = entry.consensusTimestampHash();
            final Bytes depth2Node2 = hashInternalNodeSingleChild(depth3Node1);

            final Bytes finalBlockHash = hashInternalNode(depth2Node1, depth2Node2);

            allPrevBlocksHasher.addNodeByHash(finalBlockHash.toByteArray());
            prevWrappedBlockHash = finalBlockHash;
        }

        // Serialize to the jumpstart binary format expected by the migration:
        //   8 bytes  : block number (long) — last processed block of the first half
        //   48 bytes : previous block root hash (SHA-384) — hash of that last block
        //   8 bytes  : streaming hasher leaf count (long)
        //   4 bytes  : streaming hasher hash count (int)
        //   48 bytes * hash count : streaming hasher pending subtree hashes
        final long jumpstartBlockNum = firstHalf.getLast().blockNumber();
        log.info(
                "Computed jumpstart state after processing {} (first-half) blocks; "
                        + "jumpstartBlockNum={}, prevWrappedBlockHash={}...",
                firstHalf.size(),
                jumpstartBlockNum,
                prevWrappedBlockHash.toHex().substring(0, 8));
        final var baos = new ByteArrayOutputStream();
        try (final var dout = new DataOutputStream(baos)) {
            dout.writeLong(jumpstartBlockNum);
            prevWrappedBlockHash.writeTo(dout);
            dout.writeLong(allPrevBlocksHasher.leafCount());
            final var intermediateHashes = allPrevBlocksHasher.intermediateHashingState();
            dout.writeInt(intermediateHashes.size());
            for (final var hash : intermediateHashes) {
                dout.write(hash.toByteArray());
            }
        }
        final var jumpstartBytes = baos.toByteArray();

        // Write jumpstart file to every node's cutover directory
        for (final var node : nodes) {
            final var workingDir = requireNonNull(node.metadata().workingDir());
            final var cutoverDir =
                    guaranteedExtantDir(workingDir.resolve(DATA_DIR).resolve(CUTOVER_DIR));
            final var dest = cutoverDir.resolve(JUMPSTART_FILENAME);
            Files.write(dest, jumpstartBytes);
            log.info("jumpstart.bin written to {}", dest);
        }

        // Expose the generated bytes for downstream verification
        contentsRef.set(jumpstartBytes);
        return false;
    }
}
